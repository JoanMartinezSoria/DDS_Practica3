---
title: "Actividad Evaluable 1"
output: pdf_document
date: "2026-01-19"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Análisis de logs de servidor usando R (parte II)

Aquí encontramos las librerias usadas para la realización de la Actividad.

```{r}
#install.packages("readr")
#install.packages("dplyr")
#install.packages("tidyr")
#install.packages("lubridate")
#install.packages("mltools")
#install.packages("data.table")
#install.packages("ggplot2")
#install.packages("stringr")
install.packages("summarytools")

library("readr")
library("dplyr")
library("tidyr")
library("lubridate")
library("mltools")
library("data.table")
library("ggplot2")
library("stringr")
library("summarytools")
```

## Obtención y carga de los Datos:

#### Descomprimir el fichero comprimido que contiene los registros del servidor, y a partir de los datos extraídos, cargar en data frame los registros con las peticiones servidas.

Empezamos, por descomprimir el archivo donde se encuentran nuestros datos.
```{r}
zip <- "./epa-http.zip"
unzip(zipfile = zip)
```

Como estos son logs, podemos usar la función *read_log()* para almacenarlos en formato dataframe (mas adelante se explicara el proceso de limpieza que hemos realizado)
```{r}
columnas <- c("IP", "Timestamp", "Peticion", "CodigoRespuesta", "BytesReply")
df <- read_log("epa-http.csv", col_names = columnas) %>%
  mutate(BytesReply = replace_na(BytesReply, 0)) %>% 
  mutate(Timestamp = as.POSIXct(strptime(Timestamp, format = "%d:%H:%M:%S"))) %>%
  mutate(
    Metodo = as.factor(str_split(Peticion, " ", simplify = TRUE)[,1]),
    URL = as.factor(str_split(Peticion, " ", simplify = TRUE)[,2]),
    Version_proto = as.factor(str_split(Peticion, " ", simplify = TRUE)[,3]),
  )

head(df, 10)
```

#### Incluid en el documento un apartado con la descripción de los datos analizados: fuente, tipología, descripción de la información contenida (los diferentes campos) y sus valores.

Los logs que estamos analizando se tratan de logs de un servidor que venian comprimidos en un fichero *.zip*, estos los leemos nosotros gracias a que su almacenamiento ha sido realizado mediante un fichero *.csv*.

Para tener una primera vision de los datos que hemos cargado y con los que vamos a trabajar, podemos usar la funcion *glimpse()*.

```{r}
glimpse(df)
```

Esta nos muestra la siguiente información de nuestro *dataframe*.

-   En total nuestro dataframe cuenta con 47,748 columnas.
-   Contamos con 5 columans distintas con el siguiente contenido y tipo de dato:
    -   **IP (character)**: Se trata de la IP que esta realizando la petición (no tiene por que ser siempre una IP, tambien puede ser el nombre del "Usuario" que quiere acceder a dicho recurso).
    -   **Timestamp (POSIXct)**: Es la fecha de cuando llego la peticion al servidor.
    -   **Peticion (character)**: Se trata de la peticion que se esta realizando al servidor.
    -   **CodigoRespuesta (double)**: Código que indica el estado de la petición.
    -   **BytesReply (double)**: Numero de bytes enviados por el servidor para servir la petición.
    -   **Metodo(Factor)**: Parte de la peticion que indica que su tipo (Hay 3 posibles opciones).
    -   **URL(Factor)**: Se trata de la URL del recurso al cual se esta accediendo (Hay 6561 distintos).
    -   **Version_proto(Factor)**: Version del protocolo (Solo hay dos tipos).

## Limpieza de los Datos
#### Aprovechando que los datos a analizar son los mismos de la primera práctica, para esta entrega es imprescindible que los datos estén en formato de “datos elegantes”.
Para tener los datos limpios, hemos realizado distintas correcciones respecto a como estos venian en *crudo*.
-   Con **read_csv()** los datos ya quedan bastante bien estructurados, y se nos crean ya las 5 primeras columnas iniciales que, lo unico que hemos hecho ha sido el cambio de los nombres de esta para poder identificarlo mejor.
-   Como tenemos filas donde encontramos valores de BytesReply nulos (Cuando hay una peticion con codigo de respuesta 404), los tratamos tambien para que no sean NA y los dejamos como 0, ya que al tratarse de un recurso no encontrado, el servidor no manda nada.
-   Luego para las fechas de los logs, las pasamos a un formato de fecha como lo es POSIXct. Este formato nos será util para más adelante ya que nos divide en varias partes las fechas.
-   Finalmente dividimos la columna *Peticion* (conservando esta en nuestro df) en tres columnas nuevas que son **Metodo**, **URL** y **Version_proto** y las almacenamos como Factores.

## Exploración de Datos
#### Identificar el número único de usuarios que han interactuado directamente con el servidor de forma segregada según si los usuarios han tenido algún tipo de error en las distintas peticiones ofrecidas por el servidor.

```{r}
usuarios_con_fallos <- df %>% 
  filter(CodigoRespuesta >= 400) %>%  
  distinct(IP) %>%                    
  nrow()

total_usuarios <- length(unique(df$IP))

usuarios_sin_fallos <- total_usuarios - usuarios_con_fallos

Fallos <- df %>%
  filter(CodigoRespuesta >= 400) %>%  
  distinct(CodigoRespuesta)
  
total_usuarios
usuarios_con_fallos
usuarios_sin_fallos

Fallos
```
```{r}
usuarios_fallos <- data.frame(
  Valor = c("Sin Fallos", "Con Fallos"),
  Cantidad = c(usuarios_sin_fallos, usuarios_con_fallos)
)

ggplot(usuarios_fallos, aes(x=Valor, y=Cantidad, fill=Valor)) +
  geom_col() + 
  scale_fill_manual(values = c("red", "blue") ) +
  theme_bw() +
  theme(legend.position="none")
```

## Análisis de Datos
#### Analizar los distintos tipos de peticiones HTTP (GET, POST, PUT, DELETE) gestionadas por el servidor, identificando la frecuencia de cada una de estas. Repetir el análisis, esta vez filtrando previamente aquellas peticiones correspondientes a recursos ofrecidos de tipo imagen. 

```{r}
tabla_freq <- freq(df$Metodo, report.nas = FALSE)
tabla_freq
```
```{r}
datos_pie <- as.data.frame(tabla_freq)

datos_pie <- datos_pie %>%
  mutate(Categoria = rownames(.)) %>%
  filter(Categoria != "Total") %>%
  filter(!is.na(Categoria) & Categoria != "<NA>")


ggplot(datos_pie, aes(x = "", y = Freq, fill = Categoria)) +
  geom_bar(stat = "identity", width = 1, color = "white") +
  coord_polar("y", start = 0) +
  theme_void() +
  labs(title = "Distribución de Métodos", fill = "Método")
```

```{r}
patron_imagenes <- "(?i)\\.(gif|jpg|jpeg|png|ico|bmp|svg)$"
df_imagenes <- df %>%
  filter(str_detect(URL, patron_imagenes))
tabla_freq_img <- freq(df_imagenes$Metodo, report.nas = FALSE)
tabla_freq_img

```

```{r}
datos_pie <- as.data.frame(tabla_freq_img)

datos_pie <- datos_pie %>%
  mutate(Categoria = rownames(.)) %>%
  filter(Categoria != "Total") %>%
  filter(!is.na(Categoria) & Categoria != "<NA>")


ggplot(datos_pie, aes(x = "", y = Freq, fill = Categoria)) +
  geom_bar(stat = "identity", width = 1, color = "white") +
  coord_polar("y", start = 0) +
  theme_void() +
  labs(title = "Distribución de Métodos", fill = "Método")
```
## Visualización de Resultados 
#### Generar al menos 2 gráficos distintos que permitan visualizar alguna característica relevante de los datos analizados.  

```{r fig.width=12, fig.height=6}
top_urls <- df %>%
  count(URL) %>%              
  arrange(desc(n)) %>%     
  slice_head(n = 10)%>%
  mutate(URL = as.character(URL))

ggplot(top_urls, aes(x = reorder(str_trunc(URL, 40), n), y = n)) +
  geom_col(fill = "#2E86C1") +
  coord_flip() + 
  theme_minimal() +
  labs(
    title = "Top 10 Recursos más solicitados",
    x = "Recurso (URL)",
    y = "Número de Peticiones"
  ) +
  geom_text(aes(label = n), hjust = -0.1, size = 3.5)
```
```{r fig.width=12, fig.height=6}
top_ips <- df %>%
  count(IP) %>%                 
  arrange(desc(n)) %>%          
  slice_head(n = 10)        

# 2. Generar el Gráfico
ggplot(top_ips, aes(x = reorder(IP, n), y = n)) +
  geom_col(fill = "#E67E22") +  
  coord_flip() +               
  theme_minimal() +
  labs(
    title = "Top 10 Usuarios más Activos (IPs)",
    subtitle = "Identificación de posibles bots o proxies con alto tráfico",
    x = "Dirección IP (Usuario)",
    y = "Número de Peticiones"
  ) +
  geom_text(aes(label = n), hjust = -0.1, size = 3.5, fontface = "bold")
```

#### Generar un gráfico que permita visualizar el número de peticiones servidas a lo largo del tiempo.
```{r}
peticiones_por_tiempo <- df %>%
  mutate(Hora = floor_date(Timestamp, unit = "hour")) %>% 
  count(Hora)

ggplot(peticiones_por_tiempo, aes(x = Hora, y = n)) +
  geom_line(color = "#0073C2", linewidth = 1) + 
  geom_point(color = "#0073C2", size = 2) +
  theme_minimal() +
  labs(
    title = "Evolución del Tráfico del Servidor",
    subtitle = "Número de peticiones servidas por hora",
    x = "Hora del día",
    y = "Total de Peticiones"
  ) +
  scale_x_datetime(date_labels = "%d %b %H:00", date_breaks = "4 hours")
```
```{r}
ggplot(df, aes(x = Timestamp)) +
  geom_histogram(binwidth = 3600, fill = "#0073C2", color = "white") + 
  theme_minimal() +
  labs(
    title = "Volumen de Peticiones por Hora (Histograma)",
    subtitle = "Frecuencia de peticiones agrupadas en intervalos de 1 hora",
    x = "Hora del día",
    y = "Número de Peticiones"
  ) +
  scale_x_datetime(date_labels = "%d %b %H:00", date_breaks = "4 hours")
```


## Clústering de datos
#### Utilizando un algoritmo de aprendizaje no supervisado, realizad un análisis de clústering con k-means para los datos del servidor. 

```{r}
df <- df %>% 
  mutate(Sum_char_peticion = nchar(Peticion), Sum_char_ip = nchar(IP))

df$URL <- NULL

epa_http_one_hot <- one_hot(as.data.table(df), sparsifyNAs = TRUE)

epa_http_one_hot$IP <- NULL
epa_http_one_hot$Timestamp <- NULL
epa_http_one_hot$Peticion <- NULL
```

```{r}
#clusters_h <- hclust(dist(df))
```

```{r}
#plot(clusters_h)
```

```{r}
#num_cluster <- 2
#result <- kmeans(epa_http_one_hot, centers = num_cluster, nstart = 25)
#result_no_na <- kmeans(na.omit(epa_http_one_hot), centers = num_cluster)

num_cluster <- 3
result <- kmeans(epa_http_one_hot, centers = num_cluster, nstart = 25)
result_no_na <- kmeans(na.omit(epa_http_one_hot), centers = num_cluster)

#num_cluster <- 4
#result <- kmeans(epa_http_one_hot, centers = num_cluster, nstart = 25)
#result_no_na <- kmeans(na.omit(epa_http_one_hot), centers = num_cluster)
```

```{r}
Colors = as.factor(result$cluster)

ggplot(epa_http_one_hot, aes(x = Sum_char_ip, y = BytesReply, color = Colors)) +
  geom_point(alpha = 0.5) +   
  theme_minimal() +
  labs(
    title = paste("Resultado K-Means con k =", num_cluster),
    x = "Sum_char_ip",
    y = "BytesReply",
    color = "Cluster"
  )
```

```{r}
# Variables continuas principales (ajusta nombres según tus columnas reales)
vars_interesantes <- c("BytesReply", "Sum_char_ip", "Sum_char_peticion", "CodigoRespuesta")
todas_vars <- names(epa_http_one_hot)

# Loop anidado manual
for (var_x in vars_interesantes) {
  for (var_y in todas_vars) {
    
    # Evitamos comparar una variable consigo misma
    if (var_x != var_y) {
      
      p <- ggplot(epa_http_one_hot, aes(x = .data[[var_x]], y = .data[[var_y]], color = as.factor(result$cluster))) +
        geom_point(alpha = 0.5) +
        theme_minimal() +
        labs(title = paste(var_x, "vs", var_y))
      
      print(p)
      
      # Pausa pequeña para que a R le de tiempo a renderizar si lo ves en pantalla
      Sys.sleep(0.1) 
    }
  }
}
```

